{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Szövegfeldolgozás"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 file kész van\n",
      "40 file kész van\n",
      "60 file kész van\n",
      "80 file kész van\n",
      "100 file kész van\n",
      "120 file kész van\n",
      "140 file kész van\n",
      "160 file kész van\n",
      "180 file kész van\n",
      "200 file kész van\n",
      "220 file kész van\n",
      "240 file kész van\n",
      "260 file kész van\n",
      "280 file kész van\n",
      "300 file kész van\n",
      "320 file kész van\n",
      "340 file kész van\n",
      "360 file kész van\n",
      "380 file kész van\n",
      "400 file kész van\n",
      "420 file kész van\n",
      "440 file kész van\n",
      "460 file kész van\n",
      "480 file kész van\n",
      "500 file kész van\n",
      "520 file kész van\n",
      "540 file kész van\n",
      "560 file kész van\n",
      "580 file kész van\n",
      "600 file kész van\n",
      "620 file kész van\n",
      "640 file kész van\n",
      "660 file kész van\n",
      "680 file kész van\n",
      "700 file kész van\n",
      "720 file kész van\n",
      "740 file kész van\n",
      "760 file kész van\n",
      "780 file kész van\n",
      "800 file kész van\n",
      "820 file kész van\n",
      "840 file kész van\n",
      "860 file kész van\n",
      "880 file kész van\n",
      "900 file kész van\n",
      "920 file kész van\n",
      "940 file kész van\n",
      "960 file kész van\n",
      "980 file kész van\n",
      "1000 file kész van\n",
      "1020 file kész van\n"
     ]
    }
   ],
   "source": [
    "ugyfel_list = list()\n",
    "diszpecser_list = list()\n",
    "\n",
    "count = 0\n",
    "\n",
    "line_num = 9\n",
    "greet_strip = 2\n",
    "\n",
    "for file_name in range(1,100):\n",
    "    first_n = 0\n",
    "\n",
    "    count += 1\n",
    "    if (count % 20 == 0):\n",
    "        print(\"{0} file kész van\".format(count))\n",
    "    ugyfel_flag = 0\n",
    "    diszpecser_flag = 0            \n",
    "    t_ugyf_l = list()\n",
    "    t_disz_l = list()\n",
    "    with open(\"C:\\Aktuális\\Egyetem\\Önlab\\TextGrid\\{0}.TextGrid\".format(\"%04d\" % file_name), encoding=\"utf-16-be\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            temp = line.replace('\\x00', \"\").strip()\n",
    "            if (len(temp.split()) != 0):\n",
    "\n",
    "                if (\"name\" in temp and \"diszpecser\" in temp):\n",
    "                    ugyfel_flag = 0\n",
    "                    diszpecser_flag = 1\n",
    "                elif(\"name\" in temp and \"ugyfel\" in temp):\n",
    "                    diszpecser_flag = 0\n",
    "                    ugyfel_flag = 1\n",
    "                if(temp.split()[0] == \"text\" and temp.split()[2].strip(\"\\\"\") is not \"\"):\n",
    "                    if(diszpecser_flag == 1):\n",
    "                        t_disz_l.append(temp[8:-1])\n",
    "                        # print(\"a diszpécser azt mondta: \" + temp[8:-1])\n",
    "                    elif(ugyfel_flag == 1):\n",
    "                        first_n += 1\n",
    "                        if first_n > line_num:\n",
    "                            break\n",
    "                        if first_n > greet_strip:\n",
    "                            t_ugyf_l.append(temp[8:-1])\n",
    "                        # print(\"az ugyfél azt mondta: \" + temp[8:-1])\n",
    "    if len(t_ugyf_l) == line_num-greet_strip:\n",
    "        ugyfel_list.append(t_ugyf_l)\n",
    "    diszpecser_list.append(t_disz_l)\n",
    "    \n",
    "# print(len(diszpecser_list))\n",
    "# print(diszpecser_list[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "945\n"
     ]
    }
   ],
   "source": [
    "print(len(ugyfel_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = list()\n",
    "\n",
    "with open(\"stopwords-hu.txt\", encoding=\"utf-16-be\", errors=\"ignore\") as f:\n",
    "    for line in f:\n",
    "        stop.append(line.strip())\n",
    "stop.append(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA modell alkalmazása\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a listák str-é alakítása (minden fájl egy str, az egymáshoz tartozó diszpécser és ügyfél szövegeket is konkatenálva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_complete = list()\n",
    "\n",
    "for i in range(len(ugyfel_list)):\n",
    "    temp = \"\"\n",
    "    for line in ugyfel_list[i]:\n",
    "        temp += line +\" \"\n",
    "    # for line in diszpecser_list[i]:\n",
    "    #     temp += line +\" \"\n",
    "    doc_complete.append(temp)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning is an important step before any text mining task, in this step, we will remove the punctuations, stopwords and normalize the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. fájl kész van\n",
      "1. fájl kész van\n",
      "2. fájl kész van\n",
      "3. fájl kész van\n",
      "4. fájl kész van\n",
      "5. fájl kész van\n",
      "6. fájl kész van\n",
      "7. fájl kész van\n",
      "8. fájl kész van\n",
      "9. fájl kész van\n",
      "10. fájl kész van\n",
      "11. fájl kész van\n",
      "12. fájl kész van\n",
      "13. fájl kész van\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-99-15b59513c2ca>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Magyarlánc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mdoc_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_complete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;31m# Snowball Stemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# doc_clean = [clean(doc).split() for doc in doc_complete]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-15b59513c2ca>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnormalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;31m# Magyarlánc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mdoc_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mclean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc_complete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;31m# Snowball Stemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# doc_clean = [clean(doc).split() for doc in doc_complete]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-15b59513c2ca>\u001b[0m in \u001b[0;36mclean\u001b[1;34m(doc, i)\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mstop_free\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mpunc_free\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;31m# Magyarlánc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mnormalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlanc_stem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_free\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Snowball Stemmer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[1;31m# normalized = \" \".join(stemmer.stem(word) for word in stop_free.split())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-99-15b59513c2ca>\u001b[0m in \u001b[0;36mlanc_stem\u001b[1;34m(i, string)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"in.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"w\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'java'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-Xmx1G'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-jar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'magyarlanc-3.0.jar'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-mode'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'morphparse'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-input'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'in.txt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'-output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'out.txt'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"out.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    267\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 269\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda\\lib\\subprocess.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout, endtime)\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1054\u001b[0m                 result = _winapi.WaitForSingleObject(self._handle,\n\u001b[1;32m-> 1055\u001b[1;33m                                                     timeout_millis)\n\u001b[0m\u001b[0;32m   1056\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0m_winapi\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mWAIT_TIMEOUT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1057\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "import subprocess\n",
    "\n",
    "# stop = set(stopwords.words('hungarian'))\n",
    "exclude = set(string.punctuation) \n",
    "stemmer = SnowballStemmer(\"hungarian\")\n",
    "\n",
    "def lanc_stem(i, string):\n",
    "    clean_str = list()\n",
    "    with open(\"in.txt\", \"w\",encoding = \"utf-8\", errors=\"ignore\") as f:\n",
    "        f.write(string)\n",
    "    subprocess.call(['java', '-Xmx1G', '-jar', 'magyarlanc-3.0.jar', '-mode', 'morphparse', '-input', 'in.txt', '-output', 'out.txt'])\n",
    "    with open(\"out.txt\", encoding = \"utf-8\", errors=\"ignore\") as f:\n",
    "        for line in f:\n",
    "            if(len(line.split()) != 0):\n",
    "                clean_str.append(line.split()[1])\n",
    "    print(\"{0}. fájl kész van\".format(i))\n",
    "    return clean_str\n",
    "            \n",
    "def clean(doc, i=0):\n",
    "    punc_free = ''.join(ch for ch in doc if ch not in exclude)\n",
    "    stop_free = \" \".join([i for i in punc_free.lower().split() if i not in stop])\n",
    "    # Magyarlánc\n",
    "    normalized = lanc_stem(i, stop_free)\n",
    "    # Snowball Stemmer\n",
    "    # normalized = \" \".join(stemmer.stem(word) for word in stop_free.split())\n",
    "    \n",
    "    return normalized\n",
    "# Magyarlánc\n",
    "doc_clean = [clean(doc, i) for i, doc in enumerate(doc_complete)]  \n",
    "# Snowball Stemmer\n",
    "# doc_clean = [clean(doc).split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the text documents combined is known as the corpus. To run any mathematical model on text corpus, it is a good practice to convert it into a matrix representation. LDA model looks for repeating term patterns in the entire DT matrix. Python provides many great libraries for text mining practices, “gensim” is one such clean and beautiful library to handle text data. It is scalable, robust and efficient. Following code shows how to convert a corpus into a document-term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our courpus, where every unique term is assigned an index. \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to create an object for LDA model and train it on Document-Term matrix. The training also requires few parameters as input which are explained in the above section. The gensim module allows both LDA model estimation from a training corpus and inference of topic distribution on new, unseen documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=4, id2word = dictionary, passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.019*\"tud\" + 0.017*\"szeretne\" + 0.013*\"szerződés\" + 0.010*\"száml\" + 0.010*\"kapt\"'), (1, '0.026*\"szeretne\" + 0.018*\"szerződés\" + 0.017*\"ö\" + 0.014*\"tud\" + 0.014*\"kapt\"'), (2, '0.023*\"szeretne\" + 0.017*\"szerződés\" + 0.012*\"invitel\" + 0.011*\"ö\" + 0.008*\"intern\"'), (3, '0.027*\"szeretne\" + 0.017*\"tud\" + 0.012*\"szolgáltatás\" + 0.011*\"telefon\" + 0.011*\"száml\"')]\n"
     ]
    }
   ],
   "source": [
    "print(ldamodel.print_topics(num_topics=4, num_words=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1_clean = clean(doc_complete[1]).split()\n",
    "doc1_bow =dictionary.doc2bow(doc1_clean)\n",
    "\n",
    "ldamodel.get_document_topics(doc1_bow)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
